{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import config\n",
    "\n",
    "\n",
    "def get_path(filename: str) -> tuple[str, str]:\n",
    "    return (\n",
    "        path.join(config.dataset_path, f\"{filename}_Extract.npy\"),\n",
    "        path.join(config.dataset_path, f\"{filename}.npy\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toolset\n",
    "import numpy as np\n",
    "from npy_append_array import NpyAppendArray\n",
    "\n",
    "if os.path.exists(toolset.temp_x_path()):\n",
    "    os.remove(toolset.temp_x_path())\n",
    "if os.path.exists(toolset.temp_y_path()):\n",
    "    os.remove(toolset.temp_y_path())\n",
    "\n",
    "with NpyAppendArray(toolset.temp_x_path()) as array_x,\\\n",
    "        NpyAppendArray(toolset.temp_y_path()) as array_y:\n",
    "    for name in config.file_names:\n",
    "        paths = get_path(name)\n",
    "\n",
    "        current_x = np.load(paths[0], mmap_mode=\"r\")\n",
    "        current_x = current_x / 255.0\n",
    "        current_x = np.ascontiguousarray(current_x.transpose((0, 3, 1, 2)))\n",
    "        print(\"i like among us\", current_x.shape)\n",
    "        array_x.append(current_x)\n",
    "\n",
    "        current_y = np.load(paths[1], mmap_mode=\"r\")\n",
    "        current_y = current_y.astype(float)\n",
    "        array_y.append(current_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.load(toolset.temp_x_path(), \"r\")\n",
    "data_y = np.load(toolset.temp_y_path(), \"r\")\n",
    "print(data_x.shape, data_y.shape)\n",
    "assert(data_x.shape[0] == data_y.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(config.kfold_nsplits, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: np.ndarray, label: np.ndarray) -> None:\n",
    "        self.x = data\n",
    "        self.y = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Kernel_size: size of filter block\n",
    "        # Stride: The distance between position of the filter\n",
    "        # Padding: Non-sense at border, so that all data is preserved\n",
    "        # (in case there is remainder when dividing by kernel_size or something)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.linear1 = nn.Linear(64*56*56, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        print(x.shape)\n",
    "        # Go through all layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avgpool(x) \n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if not use_cuda:\n",
    "    print(\"CUDA not used!\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_idx: np.ndarray) -> tuple[float, float]:\n",
    "    train = Data(data_x[train_idx], data_y[judge_idx])\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=config.batch_size)\n",
    "    total_loss_train = 0\n",
    "    total_accumulate_train = 0\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x.float())\n",
    "        output = output.squeeze()\n",
    "        # print(y)\n",
    "\n",
    "        # print(type(output))\n",
    "        batch_loss = criterion(output, y)\n",
    "        \n",
    "        total_loss_train += batch_loss\n",
    "\n",
    "        accumulate = (abs(output - y) <= 0.5).sum()\n",
    "        total_accumulate_train += accumulate\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss_train = total_loss_train.item()\n",
    "    total_accumulate_train = total_accumulate_train.item()\n",
    "    return (total_loss_train, total_accumulate_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(judge_idx: np.ndarray) -> tuple[float, float]:\n",
    "    judge = Data(data_x[judge_idx], data_y[judge_idx])\n",
    "    judge_dataloader = torch.utils.data.DataLoader(\n",
    "        judge, batch_size=config.batch_size)\n",
    "\n",
    "    total_loss_judge = 0\n",
    "    total_accumulate_judge = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(judge_dataloader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x.float())\n",
    "            output = output.squeeze()\n",
    "\n",
    "            batch_loss = criterion(output, y)\n",
    "            total_loss_judge += batch_loss\n",
    "\n",
    "            accumulate = (abs(output - y) <= 0.5).sum()\n",
    "            total_accumulate_judge += accumulate\n",
    "\n",
    "    total_loss_judge = total_loss_judge.item()\n",
    "    total_accumulate_judge = total_accumulate_judge.item()\n",
    "    return (total_loss_judge, total_accumulate_judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "min_judge_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch, (train_idx, judge_idx) in enumerate(kfold.split(data_x)):\n",
    "    total_loss_train, total_accumulate_train = train(train_idx)\n",
    "    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n",
    "    print(\n",
    "        f'''Epochs: {epoch+1} \n",
    "        | Train Loss: {total_loss_train / len(train_idx):.3f}\n",
    "        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n",
    "        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n",
    "        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}'''\n",
    "    )\n",
    "    if min_judge_loss > total_loss_judge/len(judge_idx):\n",
    "        min_judge_loss = total_loss_judge/len(judge_idx)\n",
    "        torch.save(model.state_dict(), \"simplemodel.pt\")\n",
    "        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
