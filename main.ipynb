{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import config\n",
    "\n",
    "\n",
    "def get_path(filename: str) -> tuple[str, str]:\n",
    "    return (\n",
    "        path.join(config.dataset_path, f\"{filename}_Extract.npy\"),\n",
    "        path.join(config.dataset_path, f\"{filename}.npy\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toolset\n",
    "import numpy as np\n",
    "from npy_append_array import NpyAppendArray\n",
    "\n",
    "if os.path.exists(toolset.temp_x_path()):\n",
    "    os.remove(toolset.temp_x_path())\n",
    "if os.path.exists(toolset.temp_y_path()):\n",
    "    os.remove(toolset.temp_y_path())\n",
    "\n",
    "with NpyAppendArray(toolset.temp_x_path()) as array_x,\\\n",
    "        NpyAppendArray(toolset.temp_y_path()) as array_y:\n",
    "    for name in config.file_names:\n",
    "        paths = get_path(name)\n",
    "\n",
    "        current_x = np.load(paths[0], mmap_mode=\"r\")\n",
    "        current_x = current_x / 255.0\n",
    "        array_x.append(current_x)\n",
    "\n",
    "        current_y = np.load(paths[1], mmap_mode=\"r\")\n",
    "        current_y = 1*current_y\n",
    "        array_y.append(current_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "data_x = np.load(toolset.temp_x_path(), \"r\")\n",
    "data_y = np.load(toolset.temp_y_path(), \"r\")\n",
    "if data_x.shape[0] != data_y.shape[0]:\n",
    "    print(\"!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(config.kfold_nsplits, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhn\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: np.ndarray, label: np.ndarray) -> None:\n",
    "        self.x = data\n",
    "        self.y = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Kernel_size: size of filter block\n",
    "        # Stride: The distance between position of the filter\n",
    "        # Padding: Non-sense at border, so that all data is preserved\n",
    "        # (in case there is remainder when dividing by kernel_size or something)\n",
    "\n",
    "        # 28x28x1 -> 26x26x32\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 13x13x32 -> 13x13x64\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=\"same\")\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # 6x6x64\n",
    "        self.linear1 = nn.Linear(6*6*64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Go through all layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:131: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  self.weight = Parameter(torch.empty(\n"
     ]
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if not use_cuda:\n",
    "    print(\"CUDA not used!\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_idx: np.ndarray) -> tuple[float, float]:\n",
    "    print(data_x.shape, data_y.shape)\n",
    "    train = Data(data_x[train_idx], data_y[judge_idx])\n",
    "    print(data_y)\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train, batch_size=config.batch_size)\n",
    "    total_loss_train = 0\n",
    "    total_accumulate_train = 0\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x.float())\n",
    "\n",
    "        batch_loss = criterion(output, y)\n",
    "        total_loss_train += batch_loss\n",
    "\n",
    "        accumulate = (output.argmax(dim=1) == y).sum()\n",
    "        total_accumulate_train += accumulate\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss_train = total_loss_train.item()\n",
    "    total_accumulate_train = total_accumulate_train.item()\n",
    "    return (total_loss_train, total_accumulate_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(judge_idx: np.ndarray) -> tuple[float, float]:\n",
    "    judge = Data(data_x[judge_idx], data_y[judge_idx])\n",
    "    judge_dataloader = torch.utils.data.DataLoader(\n",
    "        judge, batch_size=config.batch_size)\n",
    "\n",
    "    total_loss_judge = 0\n",
    "    total_accumulate_judge = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(judge_dataloader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x.float())\n",
    "\n",
    "            batch_loss = criterion(output, y)\n",
    "            total_loss_judge += batch_loss\n",
    "\n",
    "            accumulate = (output.argmax(dim=1) == y).sum()\n",
    "            total_accumulate_judge += accumulate\n",
    "\n",
    "    total_loss_judge = total_loss_judge.item()\n",
    "    total_accumulate_judge = total_accumulate_judge.item()\n",
    "    return (total_loss_judge, total_accumulate_judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(787, 144, 256, 3) (701,)\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of numpy.int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\BallPunchAIModel\\main.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=2'>3</a>\u001b[0m min_judge_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch, (train_idx, judge_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(kfold\u001b[39m.\u001b[39msplit(data_x)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=6'>7</a>\u001b[0m     total_loss_train, total_accumulate_train \u001b[39m=\u001b[39m train(train_idx)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=7'>8</a>\u001b[0m     total_loss_judge, total_accumulate_judge \u001b[39m=\u001b[39m judge(judge_idx)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=9'>10</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mEpochs: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=10'>11</a>\u001b[0m \u001b[39m        | Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss_train \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=15'>16</a>\u001b[0m \u001b[39m        | Val idx: \u001b[39m\u001b[39m{\u001b[39;00mjudge_idx\u001b[39m}\u001b[39;00m\u001b[39m'''\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000011?line=16'>17</a>\u001b[0m     )\n",
      "\u001b[1;32me:\\BallPunchAIModel\\main.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_idx)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000014?line=6'>7</a>\u001b[0m total_loss_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000014?line=7'>8</a>\u001b[0m total_accumulate_train \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000014?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000014?line=9'>10</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/main.ipynb#ch0000014?line=10'>11</a>\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:151\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[39mreturn\u001b[39;00m default_collate([torch\u001b[39m.\u001b[39mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch])\n\u001b[0;32m    150\u001b[0m     \u001b[39melif\u001b[39;00m elem\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():  \u001b[39m# scalars\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mas_tensor(batch)\n\u001b[0;32m    152\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mfloat\u001b[39m):\n\u001b[0;32m    153\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(batch, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of numpy.int32"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "min_judge_loss = float('inf')\n",
    "\n",
    "\n",
    "for epoch, (train_idx, judge_idx) in enumerate(kfold.split(data_x)):\n",
    "    total_loss_train, total_accumulate_train = train(train_idx)\n",
    "    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n",
    "    print(\n",
    "        f'''Epochs: {epoch+1} \n",
    "        | Train Loss: {total_loss_train / len(train):.3f}\n",
    "        | Train Accuracy: {total_accumulate_train/len(train):.3f}\n",
    "        | Val Loss: {total_loss_judge/len(judge):.3f}\n",
    "        | Val Accuracy: {total_accumulate_judge/len(judge):.3f}\n",
    "        | Train idx: {train_idx}\n",
    "        | Val idx: {judge_idx}'''\n",
    "    )\n",
    "    if min_judge_loss > total_loss_judge/len(judge):\n",
    "        min_judge_loss = total_loss_judge/len(judge)\n",
    "        torch.save(model.state_dict(), \"simplemodel.pt\")\n",
    "        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
