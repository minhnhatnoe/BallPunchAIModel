{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Get the dataset"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/phuonghd/.conda/envs/nhatenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["(73058, 3, 224, 224) (73058,)\n","Stats:\n","    | Number of not-punching: 38292\n","    | Number of punching: 34766\n"]}],"source":["from typing import Tuple\n","import numpy as np\n","import config\n","\n","dataset_image = np.load(config.x_path, mmap_mode=\"r\")\n","dataset_label = np.load(config.y_path, mmap_mode=\"r\")\n","print(dataset_image.shape, dataset_label.shape)\n","assert(dataset_image.shape[0] == dataset_label.shape[0])\n","print(f'''Stats:\n","    | Number of punching: {dataset_label.sum()}\n","    | Number of not-punching: {dataset_label.shape[0] - dataset_label.sum()}''')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Boilerplate Code"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import RepeatedKFold\n","import config\n","\n","kfold = RepeatedKFold(\n","    n_splits=config.kfold_nsplits,\n","    n_repeats=config.kfold_nrepeats,\n","    random_state=config.seed\n",")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","class Data(torch.utils.data.Dataset):\n","    def __init__(self, image: np.ndarray, label: np.ndarray, indices: np.ndarray) -> None:\n","        assert(image.shape[0] == label.shape[0])\n","        self.image = image\n","        self.label = label\n","        self.indices = indices\n","    \n","    def __len__(self) -> int:\n","        return self.indices.shape[0]\n","    \n","    def __getitem__(self, idx: int) -> 'Tuple[np.ndarray, bool]':\n","        idx = self.indices[idx]\n","        return self.image[idx], self.label[idx]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch import nn\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def print_model(model: nn.Module):\n","    param_size = 0\n","    for param in model.parameters():\n","        param_size += param.nelement() * param.element_size()\n","    buffer_size = 0\n","    for buffer in model.buffers():\n","        buffer_size += buffer.nelement() * buffer.element_size()\n","    \n","    param_size  = param_size / 1024**2\n","    buffer_size = buffer_size / 1024**2\n","    print(f'Param size: {param_size:.3f}MB')\n","    print(f'Buffer size: {buffer_size:.3f}MB')\n","\n","def print_tensor(tensor: torch.Tensor):\n","    size_gb = tensor.element_size() * tensor.nelement() / (1<<30)\n","    print(f\"{size_gb:.3f}GB\")\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["model = config.get_model()\n","\n","use_cuda = torch.cuda.is_available()\n","if not use_cuda:\n","    print(\"CUDA not used!\")\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","model = model.to(device)\n","criterion = criterion.to(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","def train(train_idx: np.ndarray) -> 'Tuple(float, float)':\n","    train = Data(dataset_image, dataset_label, train_idx)\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train, batch_size=config.batch_size)\n","    total_loss_train = 0\n","    total_accumulate_train = 0\n","    for image, label in tqdm(train_dataloader):\n","        image = image.to(device, dtype=torch.float)\n","        label = label.to(device, dtype=torch.uint8)\n","        \n","        output = model(image)\n","        batch_loss = criterion(output, label)\n","\n","        total_loss_train += batch_loss\n","\n","        accumulate = (output.argmax(dim=1) == label).sum()\n","        total_accumulate_train += accumulate\n","\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","    total_loss_train = total_loss_train.item()\n","    total_accumulate_train = total_accumulate_train.item()\n","    return (total_loss_train, total_accumulate_train)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> 'Tuple(float, float)':\n","    judge = Data(dataset_image, dataset_label, judge_idx)\n","    judge_dataloader = torch.utils.data.DataLoader(\n","        judge, batch_size=config.batch_size)\n","\n","    total_loss_judge = 0\n","    total_accumulate_judge = 0\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            image = image.to(device, dtype=torch.float)\n","            label = label.to(device, dtype=torch.uint8)\n","\n","            output = model(image)\n","\n","            batch_loss = criterion(output, label)\n","            total_loss_judge += batch_loss\n","\n","            accumulate = (output.argmax(dim=1) == label).sum()\n","            total_accumulate_judge += accumulate\n","\n","    total_loss_judge = total_loss_judge.item()\n","    total_accumulate_judge = total_accumulate_judge.item()\n","    return total_loss_judge, total_accumulate_judge"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting epoch 1\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/257 [00:00<?, ?it/s]/home/phuonghd/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:149: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352630480/work/torch/csrc/utils/tensor_numpy.cpp:172.)\n","  return default_collate([torch.as_tensor(b) for b in batch])\n","100%|██████████| 257/257 [02:45<00:00,  1.55it/s]\n","100%|██████████| 29/29 [00:08<00:00,  3.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 \n","        | Train Loss: 0.001\n","        | Train Accuracy: 0.895\n","        | Val Loss: 0.193\n","        | Val Accuracy: 0.528\n","Save model because val loss improve loss 0.193\n","Starting epoch 2\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:09<00:00,  1.03it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 \n","        | Train Loss: 0.004\n","        | Train Accuracy: 0.877\n","        | Val Loss: 0.181\n","        | Val Accuracy: 0.518\n","Save model because val loss improve loss 0.181\n","Starting epoch 3\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:18<00:00,  1.00s/it]\n","100%|██████████| 29/29 [00:09<00:00,  2.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3 \n","        | Train Loss: 0.004\n","        | Train Accuracy: 0.870\n","        | Val Loss: 0.128\n","        | Val Accuracy: 0.521\n","Save model because val loss improve loss 0.128\n","Starting epoch 4\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:16<00:00,  1.00it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4 \n","        | Train Loss: 0.004\n","        | Train Accuracy: 0.867\n","        | Val Loss: 0.098\n","        | Val Accuracy: 0.530\n","Save model because val loss improve loss 0.098\n","Starting epoch 5\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:15<00:00,  1.01it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5 \n","        | Train Loss: 0.004\n","        | Train Accuracy: 0.851\n","        | Val Loss: 0.081\n","        | Val Accuracy: 0.516\n","Save model because val loss improve loss 0.081\n","Starting epoch 6\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:14<00:00,  1.01it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.863\n","        | Val Loss: 0.027\n","        | Val Accuracy: 0.525\n","Save model because val loss improve loss 0.027\n","Starting epoch 7\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:15<00:00,  1.01it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7 \n","        | Train Loss: 0.002\n","        | Train Accuracy: 0.831\n","        | Val Loss: 0.027\n","        | Val Accuracy: 0.530\n","Starting epoch 8\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:16<00:00,  1.00it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 8 \n","        | Train Loss: 0.002\n","        | Train Accuracy: 0.842\n","        | Val Loss: 0.018\n","        | Val Accuracy: 0.529\n","Save model because val loss improve loss 0.018\n","Starting epoch 9\n","        | Train size: 65753\n","        | Judge size: 7305\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [04:15<00:00,  1.00it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9 \n","        | Train Loss: 0.002\n","        | Train Accuracy: 0.837\n","        | Val Loss: 0.038\n","        | Val Accuracy: 0.528\n","Starting epoch 10\n","        | Train size: 65753\n","        | Judge size: 7305\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:43<00:00,  1.15it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.835\n","        | Val Loss: 0.015\n","        | Val Accuracy: 0.516\n","Save model because val loss improve loss 0.015\n","Starting epoch 11\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:40<00:00,  1.16it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 11 \n","        | Train Loss: 0.002\n","        | Train Accuracy: 0.806\n","        | Val Loss: 0.011\n","        | Val Accuracy: 0.521\n","Save model because val loss improve loss 0.011\n","Starting epoch 12\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:40<00:00,  1.16it/s]\n","100%|██████████| 29/29 [00:09<00:00,  2.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 12 \n","        | Train Loss: 0.002\n","        | Train Accuracy: 0.759\n","        | Val Loss: 0.006\n","        | Val Accuracy: 0.525\n","Save model because val loss improve loss 0.006\n","Starting epoch 13\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:39<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 13 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.534\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.531\n","Save model because val loss improve loss 0.003\n","Starting epoch 14\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:39<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  2.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 14 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.486\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.517\n","Save model because val loss improve loss 0.003\n","Starting epoch 15\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:38<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 15 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.478\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.523\n","Save model because val loss improve loss 0.003\n","Starting epoch 16\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:39<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 16 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.477\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.512\n","Starting epoch 17\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:39<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 17 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.472\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.532\n","Save model because val loss improve loss 0.003\n","Starting epoch 18\n","        | Train size: 65752\n","        | Judge size: 7306\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:38<00:00,  1.18it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 18 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.470\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.529\n","Save model because val loss improve loss 0.003\n","Starting epoch 19\n","        | Train size: 65753\n","        | Judge size: 7305\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:38<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 19 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.468\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.521\n","Starting epoch 20\n","        | Train size: 65753\n","        | Judge size: 7305\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 257/257 [03:40<00:00,  1.17it/s]\n","100%|██████████| 29/29 [00:09<00:00,  3.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 20 \n","        | Train Loss: 0.003\n","        | Train Accuracy: 0.465\n","        | Val Loss: 0.003\n","        | Val Accuracy: 0.530\n","Save model because val loss improve loss 0.003\n"]}],"source":["min_judge_loss = float('inf')\n","\n","\n","for epoch, (train_idx, judge_idx) in enumerate(kfold.split(dataset_label)):\n","    print(f'''Starting epoch {epoch+1}\n","        | Train size: {train_idx.shape[0]}\n","        | Judge size: {judge_idx.shape[0]}''')\n","    total_loss_train, total_accumulate_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epoch: {epoch+1} \n","        | Train Loss: {total_loss_train / len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}'''\n","        \n","    )\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        torch.save(model.state_dict(), config.model_path)\n","        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('nhatenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"867ad48846dc57cdb6a7846b46ef358b3169581697c80695711b9dc516eb64da"}}},"nbformat":4,"nbformat_minor":2}
