{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import everything\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Tuple\n","from os.path import exists\n","import torch\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from torch.utils.data import DataLoader\n","from settings import cfg\n","from helper import loader\n","import export_result\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load everything"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Param size: 512.227MB\n","Buffer size: 0.032MB\n","Stats:\n","        | Number of not-punching: 34006\n","        | Number of punching: 6355\n"]}],"source":["config = cfg.TrainConfig()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","\n","def train(train_idx: np.ndarray) -> 'Tuple(float, float)':\n","    config.model.train()\n","    train_dataloader = config.get_dataloader(train_idx)\n","    total_loss_train = 0\n","\n","    prediction_array = []\n","    label_array = []\n","    for image, label in tqdm(train_dataloader):\n","        loader.to_device([image, label], config.device)\n","        image = config.transforms(image)\n","\n","        output = config.model(image)\n","        batch_loss = config.criterion(output, label)\n","        total_loss_train += batch_loss.item()\n","\n","        prediction = output.argmax(dim=1)\n","        prediction_array.append(prediction.cpu().numpy())\n","        label_array.append(label.cpu().numpy())\n","\n","        config.optimizer.zero_grad()\n","        batch_loss.backward()\n","        config.optimizer.step()\n","        config.model.zero_grad()\n","\n","    prediction_array = np.concatenate(prediction_array)\n","    label_array = np.concatenate(label_array)\n","\n","    total_accumulate_train = (prediction_array == label_array).sum().item()\n","    f1_score_train = f1_score(label_array, prediction_array, average='macro')\n","\n","    return total_loss_train, total_accumulate_train, f1_score_train\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> 'Tuple(float, float)':\n","    config.model.eval()\n","    judge_dataloader = config.get_dataloader(judge_idx)\n","    total_loss_judge = 0\n","\n","    prediction_array = []\n","    label_array = []\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            loader.to_device([image, label], config.device)\n","\n","            output = config.model(image)\n","            batch_loss = config.criterion(output, label)\n","            total_loss_judge += batch_loss.item()\n","\n","            prediction = output.argmax(dim=1)\n","            prediction_array.append(prediction.cpu().numpy())\n","            label_array.append(label.cpu().numpy())\n","\n","    prediction_array = np.concatenate(prediction_array)\n","    label_array = np.concatenate(label_array)\n","\n","    total_accumulate_judge = (prediction_array == label_array).sum().item()\n","    f1_score_judge = f1_score(label_array, prediction_array, average='macro')\n","\n","    return total_loss_judge, total_accumulate_judge, f1_score_judge"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting phase 1\n","Starting epoch 1\n","        | Train size: 36324\n","        | Judge size: 4037\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb0e909c2b274ec88424057a502f5ef7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1136 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["cuda\n","torch.Size([32, 3, 224, 224]) <class 'torch.Tensor'> torch.float32 cpu\n"]},{"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch, (train_idx, judge_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(config\u001b[39m.\u001b[39mget_split()):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mStarting epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m \u001b[39m        | Train size: \u001b[39m\u001b[39m{\u001b[39;00mtrain_idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=8'>9</a>\u001b[0m \u001b[39m        | Judge size: \u001b[39m\u001b[39m{\u001b[39;00mjudge_idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'''\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=9'>10</a>\u001b[0m     total_loss_train, total_accumulate_train, f1_score_train \u001b[39m=\u001b[39m train(train_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=10'>11</a>\u001b[0m     total_loss_judge, total_accumulate_judge, f1_score_judge \u001b[39m=\u001b[39m judge(judge_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=13'>14</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=14'>15</a>\u001b[0m \u001b[39m        | Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss_train\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_idx)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=17'>18</a>\u001b[0m \u001b[39m        | Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss_judge\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(judge_idx)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=18'>19</a>\u001b[0m \u001b[39m        | Val F1 Score: \u001b[39m\u001b[39m{\u001b[39;00mf1_score_judge\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'''\u001b[39m)\n","\u001b[1;32m/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(image\u001b[39m.\u001b[39mshape, \u001b[39mtype\u001b[39m(image), image\u001b[39m.\u001b[39mdtype, image\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=15'>16</a>\u001b[0m image \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mtransforms(image)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=17'>18</a>\u001b[0m output \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mmodel(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=18'>19</a>\u001b[0m batch_loss \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mcriterion(output, label)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000007vscode-remote?line=19'>20</a>\u001b[0m total_loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem()\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/nn/modules/container.py:154\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    153\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1186\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1185\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1186\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1187\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"]}],"source":["min_judge_loss = float('inf')\n","last_submit = 0\n","\n","print(\"Starting phase 1\")\n","\n","for epoch, (train_idx, judge_idx) in enumerate(config.get_split()):\n","    print(f'''Starting epoch {epoch+1}\n","        | Train size: {train_idx.shape[0]}\n","        | Judge size: {judge_idx.shape[0]}''')\n","    total_loss_train, total_accumulate_train, f1_score_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge, f1_score_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epoch: {epoch+1} \n","        | Train Loss: {total_loss_train/len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Train F1 Score: {f1_score_train:.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val F1 Score: {f1_score_judge:.3f}''')\n","    \n","    if last_submit == 0:\n","        last_submit = f1_score_judge\n","    elif f1_score_judge - last_submit > 0.05:\n","        last_submit = f1_score_judge\n","        export_result.submit(config)\n","\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        config.save_checkpoint()\n","        print(\n","            f\"Save model because val loss improve loss {min_judge_loss:.3f}\")\n","        under_min = 0\n","    else:\n","        under_min += 1\n","        if under_min > cfg.early_stop:\n","            print(\n","                f\"Early stop because val loss not improve {under_min} epochs. Resetting model.\")\n","            config.load_best()\n","            under_min = 0\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('nhatenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"867ad48846dc57cdb6a7846b46ef358b3169581697c80695711b9dc516eb64da"}}},"nbformat":4,"nbformat_minor":2}
