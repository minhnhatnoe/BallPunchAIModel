{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Get the dataset"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 3, 224, 224) (10,)\n"]}],"source":["import numpy as np\n","import config\n","\n","dataset_image = np.load(config.x_path, mmap_mode=\"r\")\n","dataset_label = np.load(config.y_path, mmap_mode=\"r\")\n","print(dataset_image.shape, dataset_label.shape)\n","assert(dataset_image.shape[0] == dataset_label.shape[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Boilerplate Code"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import KFold\n","import config\n","\n","kfold = KFold(config.kfold_nsplits, shuffle=True, random_state=config.seed)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from typing import Tuple\n","import torch\n","\n","class Data(torch.utils.data.Dataset):\n","    def __init__(self, image: np.ndarray, label: np.ndarray) -> None:\n","        assert(image.shape[0] == label.shape[0])\n","        self.image = image\n","        self.label = label\n","    \n","    def __iter__(self):\n","        for x, y in zip(self.image, self.label):\n","            yield x, y\n","    \n","    def __len__(self):\n","        return self.image.shape[0]\n","    \n","    def __getitem__(self, idx: int):\n","        return self.image[idx], self.label[idx]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch import nn\n","from torchvision.models import vgg16, VGG16_Weights\n","\n","\n","def get_model() -> nn.Module:\n","    model = vgg16(weights=VGG16_Weights.DEFAULT, progress=True)\n","    in_features = model.classifier[6].in_features\n","    model.classifier[6] = nn.Linear(in_features, 2)\n","    return model\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model = get_model()\n","\n","use_cuda = torch.cuda.is_available()\n","if not use_cuda:\n","    print(\"CUDA not used!\")\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","model = model.to(device)\n","criterion = criterion.to(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32me:\\BallPunchAIModel\\src\\model.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(train_idx: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple(\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000009?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTRAINING\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000009?line=3'>4</a>\u001b[0m     train \u001b[39m=\u001b[39m Data(dataset_image[train_idx], dataset_label[train_idx])\n","\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["from tqdm import tqdm\n","def train(train_idx: np.ndarray) -> Tuple(float, float):\n","    print(\"TRAINING\")\n","    train = Data(dataset_image[train_idx], dataset_label[train_idx])\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train, batch_size=config.batch_size)\n","    print(\"MADE DATALOADER\")\n","    total_loss_train = 0\n","    total_accumulate_train = 0\n","    for image, label in tqdm(train_dataloader):\n","        image = image.to(device, dtype=torch.float)\n","        label = label.to(device, dtype=torch.uint8)\n","        output = model(image)\n","        print(\"OUT: \", output)\n","        print(\"LABEL: \", label)\n","\n","        batch_loss = criterion(output, label)\n","\n","        total_loss_train += batch_loss\n","\n","        accumulate = (output.argmax(dim=1) == label).sum()\n","        print(accumulate, type(accumulate))\n","        total_accumulate_train += accumulate\n","\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","    total_loss_train = total_loss_train.item()\n","    total_accumulate_train = total_accumulate_train.item()\n","    return (total_loss_train, total_accumulate_train)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> Tuple(float, float):\n","    judge = Data(dataset_image[judge_idx], dataset_label[judge_idx])\n","    judge_dataloader = torch.utils.data.DataLoader(\n","        judge, batch_size=config.batch_size)\n","\n","    total_loss_judge = 0\n","    total_accumulate_judge = 0\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            image = image.to(device, dtype=torch.float)\n","            label = label.to(device, dtype=torch.uint8)\n","\n","            output = model(image)\n","\n","            batch_loss = criterion(output, label)\n","            total_loss_judge += batch_loss\n","\n","            accumulate = (output.argmax(dim=1) == label).sum()\n","            print(accumulate, type(accumulate))\n","            total_accumulate_judge += accumulate\n","\n","    total_loss_judge = total_loss_judge.item()\n","    total_accumulate_judge = total_accumulate_judge.item()\n","    return total_loss_judge, total_accumulate_judge"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","TRAINING\n","MADE DATALOADER\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["OUT:  tensor([[ 0.8105,  0.0932],\n","        [ 0.6063,  1.0488],\n","        [ 1.0895,  0.3594],\n","        [-0.0539,  0.0170],\n","        [ 0.8855,  0.4818],\n","        [-1.0410, -0.0190],\n","        [ 0.6168,  0.2529],\n","        [-0.0567, -0.1889],\n","        [ 0.5724,  0.2531]], device='cuda:0', grad_fn=<AddmmBackward0>)\n","LABEL:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n","tensor(4, device='cuda:0') <class 'torch.Tensor'>\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:03<00:00,  3.49s/it]\n","100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["tensor(1, device='cuda:0') <class 'torch.Tensor'>\n","Epochs: 1 \n","        | Train Loss: 0.079\n","        | Train Accuracy: 0.444\n","        | Val Loss: 0.000\n","        | Val Accuracy: 1.000\n","Save model because val loss improve loss 0.000\n","1\n","TRAINING\n","MADE DATALOADER\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["OUT:  tensor([[-32.0443,  33.5971],\n","        [-38.9766,  42.6384],\n","        [-23.7156,  40.5637],\n","        [-27.0355,  27.9077],\n","        [-18.0297,  31.6772],\n","        [-37.2963,  32.7003],\n","        [-37.7067,  34.8429],\n","        [-27.3638,  32.7682],\n","        [-27.6980,  35.8973]], device='cuda:0', grad_fn=<AddmmBackward0>)\n","LABEL:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n","tensor(8, device='cuda:0') <class 'torch.Tensor'>\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 4.00 GiB total capacity; 2.39 GiB already allocated; 0 bytes free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\BallPunchAIModel\\src\\model.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch, (train_idx, judge_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(kfold\u001b[39m.\u001b[39msplit(dataset_label)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(epoch)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=5'>6</a>\u001b[0m     total_loss_train, total_accumulate_train \u001b[39m=\u001b[39m train(train_idx)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=6'>7</a>\u001b[0m     total_loss_judge, total_accumulate_judge \u001b[39m=\u001b[39m judge(judge_idx)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=9'>10</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mEpochs: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=10'>11</a>\u001b[0m \u001b[39m        | Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss_train \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_idx)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=14'>15</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=15'>16</a>\u001b[0m     )\n","\u001b[1;32me:\\BallPunchAIModel\\src\\model.ipynb Cell 12\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=24'>25</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=25'>26</a>\u001b[0m     batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=26'>27</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=28'>29</a>\u001b[0m total_loss_train \u001b[39m=\u001b[39m total_loss_train\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BallPunchAIModel/src/model.ipynb#ch0000011?line=29'>30</a>\u001b[0m total_accumulate_train \u001b[39m=\u001b[39m total_accumulate_train\u001b[39m.\u001b[39mitem()\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n","File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:307\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    305\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    306\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    309\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 4.00 GiB total capacity; 2.39 GiB already allocated; 0 bytes free; 3.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["\n","min_judge_loss = float('inf')\n","\n","\n","for epoch, (train_idx, judge_idx) in enumerate(kfold.split(dataset_label)):\n","    print(epoch)\n","    total_loss_train, total_accumulate_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epochs: {epoch+1} \n","        | Train Loss: {total_loss_train / len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}'''\n","        \n","    )\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        torch.save(model.state_dict(), \"simplemodel.pt\")\n","        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":2}
