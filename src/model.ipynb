{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import everything\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialized\n"]}],"source":["from typing import Tuple\n","import torch\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from torch.utils.data import DataLoader\n","from settings import cfg\n","from helper import boilerplate, debug, loader\n","from os.path import exists\n"]},{"cell_type":"markdown","metadata":{},"source":["# Get the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Stats:\n","    | Number of punching: 6355\n","    | Number of not-punching: 34006\n"]}],"source":["dataset_image, dataset_label = loader.load_data(cfg.train_paths, mmap_mode='c')\n","punching = dataset_label.sum()\n","not_punching = dataset_label.shape[0] - punching\n","print(f'''Stats:\n","    | Number of punching: {punching}\n","    | Number of not-punching: {not_punching}''')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Boilerplate Code"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["Data = boilerplate.TrainingDataset\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Param size: 512.227MB\n","Buffer size: 0.032MB\n"]}],"source":["from settings.cfg import device, model, transforms, optimizer, batch_size, get_idx_gen\n","\n","criterion = cfg.get_loss([not_punching, punching])\n","debug.print_model_size(model)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","\n","def train(train_idx: np.ndarray) -> 'Tuple(float, float)':\n","    model.train()\n","    train = Data(dataset_image, dataset_label, train_idx)\n","    train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n","    total_loss_train = 0\n","\n","    prediction_array = []\n","    label_array = []\n","    for image, label in tqdm(train_dataloader):\n","        image = image.to(device, dtype=torch.float)\n","        label = label.to(device, dtype=torch.uint8)\n","\n","        image = transforms(image)\n","        # debug.print_image(image)\n","        output = model(image)\n","\n","        batch_loss = criterion(output, label)\n","        total_loss_train += batch_loss.item()\n","\n","        prediction = output.argmax(dim=1)\n","\n","        prediction_array.append(prediction.cpu().numpy())\n","        label_array.append(label.cpu().numpy())\n","\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","        model.zero_grad()\n","\n","    prediction_array = np.concatenate(prediction_array)\n","    label_array = np.concatenate(label_array)\n","\n","    total_accumulate_train = (prediction_array == label_array).sum().item()\n","    f1_score_train = f1_score(label_array, prediction_array, average='macro')\n","\n","    return total_loss_train, total_accumulate_train, f1_score_train\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> 'Tuple(float, float)':\n","    model.eval()\n","    judge = Data(dataset_image, dataset_label, judge_idx)\n","    judge_dataloader = DataLoader(judge, batch_size=batch_size)\n","    total_loss_judge = 0\n","\n","    prediction_array = []\n","    label_array = []\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            image = image.to(device, dtype=torch.float)\n","            label = label.to(device, dtype=torch.uint8)\n","\n","            output = model(image)\n","            batch_loss = criterion(output, label)\n","            total_loss_judge += batch_loss.item()\n","\n","            prediction = output.argmax(dim=1)\n","            prediction_array.append(prediction.cpu().numpy())\n","            label_array.append(label.cpu().numpy())\n","\n","    prediction_array = np.concatenate(prediction_array)\n","    label_array = np.concatenate(label_array)\n","\n","    total_accumulate_judge = (prediction_array == label_array).sum().item()\n","    f1_score_judge = f1_score(label_array, prediction_array, average='macro')\n","\n","    return total_loss_judge, total_accumulate_judge, f1_score_judge"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model\n","Starting phase 1\n","Starting epoch 1\n","        | Train size: 36324\n","        | Judge size: 4037\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"474990ec09cf4f39ab199a67167f9718","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1136 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c1039c0acc3434fb0065adaed7845ab","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/127 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 \n","        | Train Loss: 0.016\n","        | Train Accuracy: 0.759\n","        | Train F1 Score: 0.664\n","        | Val Loss: 0.017\n","        | Val Accuracy: 0.818\n","        | Val F1 Score: 0.700\n","Starting phase 2\n","Starting epoch 1\n","        | Train size: 36324\n","        | Judge size: 4037\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2146785012294307ae1036380327e7fc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1136 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"144f89282f4d431da786ec6d5b1aa243","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/127 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 \n","        | Train Loss: 0.016\n","        | Train Accuracy: 0.776\n","        | Train F1 Score: 0.679\n","        | Val Loss: 0.016\n","        | Val Accuracy: 0.801\n","        | Val F1 Score: 0.702\n","Save model because val f1 score improve 0.702\n","Starting epoch 2\n","        | Train size: 36325\n","        | Judge size: 4036\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89b778b8c5de49ff8d938eb6a0873c1b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1136 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75f295c8a4824c8abb245bc17f9b68c5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/127 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 2 \n","        | Train Loss: 0.015\n","        | Train Accuracy: 0.778\n","        | Train F1 Score: 0.687\n","        | Val Loss: 0.018\n","        | Val Accuracy: 0.843\n","        | Val F1 Score: 0.723\n","Save model because val f1 score improve 0.723\n","Starting epoch 3\n","        | Train size: 36325\n","        | Judge size: 4036\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3091080d95e14c3d98df4ee6c92506f8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1136 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["min_judge_loss = float('inf')\n","max_f1 = 0.0\n","f1_targ = 0.6\n","phase2 = False\n","under_min = 0\n","if exists(cfg.model_path):\n","    print(\"Loading model\")\n","    model.load_state_dict(torch.load(cfg.model_path))\n","\n","print(\"Starting phase 1\")\n","\n","for epoch, (train_idx, judge_idx) in enumerate(get_idx_gen().split(dataset_label)):\n","    print(f'''Starting epoch {epoch+1}\n","        | Train size: {train_idx.shape[0]}\n","        | Judge size: {judge_idx.shape[0]}''')\n","    total_loss_train, total_accumulate_train, f1_score_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge, f1_score_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epoch: {epoch+1} \n","        | Train Loss: {total_loss_train/len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Train F1 Score: {f1_score_train:.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}\n","        | Val F1 Score: {f1_score_judge:.3f}''')\n","\n","    if f1_score_judge > f1_targ:\n","        under_min = 0\n","        max_f1 = f1_score_judge\n","        break\n","\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        torch.save(model.state_dict(), cfg.model_path)\n","        print(\n","            f\"Save model because val loss improve loss {min_judge_loss:.3f}\")\n","        under_min = 0\n","    else:\n","        under_min += 1\n","        if under_min > cfg.early_stop:\n","            print(\n","                f\"Early stop because val loss not improve {under_min} epochs. Resetting model.\")\n","            model.load_state_dict(torch.load(cfg.model_path))\n","            under_min = 0\n","\n","print(\"Starting phase 2\")\n","\n","for epoch, (train_idx, judge_idx) in enumerate(get_idx_gen().split(dataset_label)):\n","    print(f'''Starting epoch {epoch+1}\n","        | Train size: {train_idx.shape[0]}\n","        | Judge size: {judge_idx.shape[0]}''')\n","    total_loss_train, total_accumulate_train, f1_score_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge, f1_score_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epoch: {epoch+1} \n","        | Train Loss: {total_loss_train/len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Train F1 Score: {f1_score_train:.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}\n","        | Val F1 Score: {f1_score_judge:.3f}''')\n","\n","    if max_f1 < f1_score_judge:\n","        max_f1 = f1_score_judge\n","        torch.save(model.state_dict(), cfg.model_path)\n","        print(f\"Save model because val f1 score improve {max_f1:.3f}\")\n","    else:\n","        under_min += 1\n","        if under_min > cfg.early_stop:\n","            print(\n","                f\"Early stop because val f1 score not improve {under_min} epochs. Resetting model.\")\n","            model.load_state_dict(torch.load(cfg.model_path))\n","            under_min = 0\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('nhatenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"867ad48846dc57cdb6a7846b46ef358b3169581697c80695711b9dc516eb64da"}}},"nbformat":4,"nbformat_minor":2}
