{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import everything\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialized\n"]}],"source":["from typing import Tuple\n","import torch\n","import numpy as np\n","from torch import nn\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from settings import cfg\n","from helper import boilerplate, debug, loader\n"]},{"cell_type":"markdown","metadata":{},"source":["# Get the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Stats:\n","    | Number of punching: 20332\n","    | Number of not-punching: 20332\n"]}],"source":["dataset_image, dataset_label = loader.load_data(cfg.train_paths, mmap_mode='c')\n","print(f'''Stats:\n","    | Number of punching: {dataset_label.sum()}\n","    | Number of not-punching: {dataset_label.shape[0] - dataset_label.sum()}''')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Boilerplate Code"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["Data = boilerplate.TrainingDataset\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Param size: 491.257MB\n","Buffer size: 0.021MB\n"]}],"source":["from settings.cfg import device, model, transforms, criterion, optimizer, idx_gen, batch_size\n","\n","debug.print_model_size(model)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def train(train_idx: np.ndarray) -> 'Tuple(float, float)':\n","    model.train()\n","    train = Data(dataset_image, dataset_label, train_idx)\n","    train_dataloader = DataLoader(train, batch_size=batch_size)\n","    total_loss_train = 0\n","    total_accumulate_train = 0\n","    for image, label in tqdm(train_dataloader):\n","        image = image.to(device, dtype=torch.float)\n","        label = label.to(device, dtype=torch.uint8)\n","\n","        image = transforms(image)\n","        output = model(image)\n","\n","        batch_loss = criterion(output, label)\n","        total_loss_train += batch_loss.item()\n","\n","        accumulate = (output.argmax(dim=1) == label).sum()\n","        total_accumulate_train += accumulate.item()\n","\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","        model.zero_grad()\n","\n","    return total_loss_train, total_accumulate_train\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> 'Tuple(float, float)':\n","    model.eval()\n","    judge = Data(dataset_image, dataset_label, judge_idx)\n","    judge_dataloader = DataLoader(judge, batch_size=batch_size)\n","\n","    total_loss_judge = 0\n","    total_accumulate_judge = 0\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            image = image.to(device, dtype=torch.float)\n","            label = label.to(device, dtype=torch.uint8)\n","\n","            output = model(image)\n","            batch_loss = criterion(output, label)\n","            total_loss_judge += batch_loss.item()\n","\n","            accumulate = (output.argmax(dim=1) == label).sum()\n","            total_accumulate_judge += accumulate.item()\n","\n","    return total_loss_judge, total_accumulate_judge"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting epoch 1\n","        | Train size: 36597\n","        | Judge size: 4067\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 572/572 [01:15<00:00,  7.54it/s]\n","100%|██████████| 64/64 [00:06<00:00,  9.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 \n","        | Train Loss: 0.005\n","        | Train Accuracy: 0.867\n","        | Val Loss: 0.979\n","        | Val Accuracy: 0.501\n"]},{"ename":"RuntimeError","evalue":"Parent directory /home/phuonghd/NHAT/BallPunchAIModel/result does not exist.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000011vscode-remote?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m min_judge_loss \u001b[39m>\u001b[39m total_loss_judge\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(judge_idx):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000011vscode-remote?line=18'>19</a>\u001b[0m     min_judge_loss \u001b[39m=\u001b[39m total_loss_judge\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(judge_idx)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000011vscode-remote?line=19'>20</a>\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), cfg\u001b[39m.\u001b[39;49mmodel_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000011vscode-remote?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSave model because val loss improve loss \u001b[39m\u001b[39m{\u001b[39;00mmin_judge_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/serialization.py:387\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    384\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    388\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    389\u001b[0m         \u001b[39mreturn\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/serialization.py:279\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 279\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n","File \u001b[0;32m~/.conda/envs/nhatenv/lib/python3.10/site-packages/torch/serialization.py:257\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_writer_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory /home/phuonghd/NHAT/BallPunchAIModel/result does not exist."]}],"source":["min_judge_loss = float('inf')\n","\n","\n","for epoch, (train_idx, judge_idx) in enumerate(idx_gen.split(dataset_label)):\n","    print(f'''Starting epoch {epoch+1}\n","        | Train size: {train_idx.shape[0]}\n","        | Judge size: {judge_idx.shape[0]}''')\n","    total_loss_train, total_accumulate_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epoch: {epoch+1} \n","        | Train Loss: {total_loss_train / len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}'''\n","    )\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        torch.save(model.state_dict(), cfg.model_path)\n","        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('nhatenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"867ad48846dc57cdb6a7846b46ef358b3169581697c80695711b9dc516eb64da"}}},"nbformat":4,"nbformat_minor":2}
