{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Get the dataset"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Stats:\n","    | Number of punching: 206\n","    | Number of not-punching: 3890\n"]}],"source":["from typing import Tuple\n","import numpy as np\n","import config\n","\n","dataset_image = np.load(config.x_path, mmap_mode='c')[0:4096]\n","dataset_label = np.load(config.y_path, mmap_mode='c')[0:4096]\n","assert(dataset_image.shape[0] == dataset_label.shape[0])\n","print(f'''Stats:\n","    | Number of punching: {dataset_label.sum()}\n","    | Number of not-punching: {dataset_label.shape[0] - dataset_label.sum()}''')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Boilerplate Code"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import RepeatedKFold\n","import config\n","\n","kfold = RepeatedKFold(\n","    n_splits=config.kfold_nsplits,\n","    n_repeats=config.kfold_nrepeats,\n","    random_state=config.seed\n",")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","class Data(torch.utils.data.Dataset):\n","    def __init__(self, image: np.ndarray, label: np.ndarray, indices: np.ndarray) -> None:\n","        assert(image.shape[0] == label.shape[0])\n","        self.image = image\n","        self.label = label\n","        self.indices = indices\n","    \n","    def __len__(self) -> int:\n","        return self.indices.shape[0]\n","    \n","    def __getitem__(self, idx: int) -> 'Tuple[np.ndarray, bool]':\n","        idx = self.indices[idx]\n","        return self.image[idx], self.label[idx]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from torch import nn\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def print_model(model: nn.Module):\n","    param_size = 0\n","    for param in model.parameters():\n","        param_size += param.nelement() * param.element_size()\n","    buffer_size = 0\n","    for buffer in model.buffers():\n","        buffer_size += buffer.nelement() * buffer.element_size()\n","    \n","    param_size  = param_size / 1024**2\n","    buffer_size = buffer_size / 1024**2\n","    print(f'Param size: {param_size:.3f}MB')\n","    print(f'Buffer size: {buffer_size:.3f}MB')\n","\n","def print_tensor(tensor: torch.Tensor):\n","    size_gb = tensor.element_size() * tensor.nelement() / (1<<30)\n","    print(f\"{size_gb:.3f}GB\")\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["model = config.get_model()\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","model = model.to(config.device)\n","criterion = criterion.to(config.device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","def train(train_idx: np.ndarray) -> 'Tuple(float, float)':\n","    train = Data(dataset_image, dataset_label, train_idx)\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train, batch_size=config.batch_size)\n","    total_loss_train = 0\n","    total_accumulate_train = 0\n","    model.train()\n","    for image, label in tqdm(train_dataloader):\n","        image = image.to(config.device, dtype=torch.float)\n","        label = label.to(config.device, dtype=torch.uint8)\n","        image = config.random_transforms(image)\n","        output = model(image)\n","\n","        batch_loss = criterion(output, label)\n","\n","        if np.isnan(batch_loss.detach().cpu().numpy()).sum():\n","            print(batch_loss, total_loss_train, label)\n","            return\n","            \n","        total_loss_train += batch_loss.item()\n","\n","        accumulate = (output.argmax(dim=1) == label).sum()\n","        total_accumulate_train += accumulate.item()\n","\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","        model.zero_grad()\n","\n","    print(total_loss_train, total_accumulate_train)\n","\n","    total_loss_train = total_loss_train.item()\n","    total_accumulate_train = total_accumulate_train.item()\n","    \n","    return (total_loss_train, total_accumulate_train)\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> 'Tuple(float, float)':\n","    judge = Data(dataset_image, dataset_label, judge_idx)\n","    judge_dataloader = torch.utils.data.DataLoader(\n","        judge, batch_size=config.batch_size)\n","\n","    total_loss_judge = 0\n","    total_accumulate_judge = 0\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            image = image.to(config.device, dtype=torch.float)\n","            label = label.to(config.device, dtype=torch.uint8)\n","\n","            output = model(image)\n","            batch_loss = criterion(output, label)\n","            total_loss_judge += batch_loss.item()\n","\n","            accumulate = (output.argmax(dim=1) == label).sum()\n","            total_accumulate_judge += accumulate.item()\n","\n","    total_loss_judge = total_loss_judge.item()\n","    total_accumulate_judge = total_accumulate_judge.item()\n","    return total_loss_judge, total_accumulate_judge"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting epoch 1\n","        | Train size: 3686\n","        | Judge size: 410\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 25/3686 [00:03<07:29,  8.14it/s] \n"]},{"name":"stdout","output_type":"stream","text":["tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>) 15.21963266655802 tensor([0], device='cuda:0', dtype=torch.uint8)\n"]},{"ename":"TypeError","evalue":"cannot unpack non-iterable NoneType object","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch, (train_idx, judge_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(kfold\u001b[39m.\u001b[39msplit(dataset_label)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mStarting epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m \u001b[39m        | Train size: \u001b[39m\u001b[39m{\u001b[39;00mtrain_idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=6'>7</a>\u001b[0m \u001b[39m        | Judge size: \u001b[39m\u001b[39m{\u001b[39;00mjudge_idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'''\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=7'>8</a>\u001b[0m     total_loss_train, total_accumulate_train \u001b[39m=\u001b[39m train(train_idx)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=8'>9</a>\u001b[0m     total_loss_judge, total_accumulate_judge \u001b[39m=\u001b[39m judge(judge_idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=11'>12</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m        | Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss_train \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_idx)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=16'>17</a>\u001b[0m         \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B103.143.207.68/home/phuonghd/NHAT/BallPunchAIModel/src/model.ipynb#ch0000012vscode-remote?line=17'>18</a>\u001b[0m     )\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"]}],"source":["min_judge_loss = float('inf')\n","\n","\n","for epoch, (train_idx, judge_idx) in enumerate(kfold.split(dataset_label)):\n","    print(f'''Starting epoch {epoch+1}\n","        | Train size: {train_idx.shape[0]}\n","        | Judge size: {judge_idx.shape[0]}''')\n","    total_loss_train, total_accumulate_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epoch: {epoch+1} \n","        | Train Loss: {total_loss_train / len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}'''\n","        \n","    )\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        torch.save(model.state_dict(), config.model_path)\n","        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('nhatenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"867ad48846dc57cdb6a7846b46ef358b3169581697c80695711b9dc516eb64da"}}},"nbformat":4,"nbformat_minor":2}
