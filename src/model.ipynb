{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Get the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import config\n","\n","dataset_image = np.load(config.x_path)\n","dataset_label = np.load(config.y_path)\n","print(dataset_image.shape, dataset_label.shape)\n","assert(dataset_image.shape[0] == dataset_label.shape[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Boilerplate Code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import RepeatedKFold\n","import config\n","\n","kfold = RepeatedKFold(\n","    n_splits=config.kfold_nsplits,\n","    n_repeats=config.kfold_nrepeats,\n","    random_state=config.seed\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Tuple\n","import torch\n","from torch import nn\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def print_model(model: nn.Module):\n","    param_size = 0\n","    for param in model.parameters():\n","        param_size += param.nelement() * param.element_size()\n","    buffer_size = 0\n","    for buffer in model.buffers():\n","        buffer_size += buffer.nelement() * buffer.element_size()\n","    \n","    param_size  = param_size / 1024**2\n","    buffer_size = buffer_size / 1024**2\n","    print(f'Param size: {param_size:.3f}MB')\n","    print(f'Buffer size: {buffer_size:.3f}MB')\n","\n","def print_tensor(tensor: torch.Tensor):\n","    size_gb = tensor.element_size() * tensor.nelement() / (1<<30)\n","    print(f\"{size_gb:.3f}GB\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = config.get_model()\n","\n","use_cuda = torch.cuda.is_available()\n","if not use_cuda:\n","    print(\"CUDA not used!\")\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters())\n","\n","model = model.to(device)\n","criterion = criterion.to(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","def train(train_idx: np.ndarray) -> 'Tuple(float, float)':\n","    train = config.Data(dataset_image, dataset_label, train_idx)\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train, batch_size=config.batch_size)\n","    total_loss_train = 0\n","    total_accumulate_train = 0\n","    for image, label in tqdm(train_dataloader):\n","        image = image.to(device, dtype=torch.float)\n","        label = label.to(device, dtype=torch.uint8)\n","        \n","        output = model(image)\n","        batch_loss = criterion(output, label)\n","\n","        total_loss_train += batch_loss\n","\n","        accumulate = (output.argmax(dim=1) == label).sum()\n","        total_accumulate_train += accumulate\n","\n","        optimizer.zero_grad()\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","    total_loss_train = total_loss_train.item()\n","    total_accumulate_train = total_accumulate_train.item()\n","    return (total_loss_train, total_accumulate_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def judge(judge_idx: np.ndarray) -> 'Tuple(float, float)':\n","    judge = config.Data(dataset_image, dataset_label, judge_idx)\n","    judge_dataloader = torch.utils.data.DataLoader(\n","        judge, batch_size=config.batch_size)\n","\n","    total_loss_judge = 0\n","    total_accumulate_judge = 0\n","    with torch.no_grad():\n","        for image, label in tqdm(judge_dataloader):\n","            image = image.to(device, dtype=torch.float)\n","            label = label.to(device, dtype=torch.uint8)\n","\n","            output = model(image)\n","\n","            batch_loss = criterion(output, label)\n","            total_loss_judge += batch_loss\n","\n","            accumulate = (output.argmax(dim=1) == label).sum()\n","            total_accumulate_judge += accumulate\n","\n","    total_loss_judge = total_loss_judge.item()\n","    total_accumulate_judge = total_accumulate_judge.item()\n","    return total_loss_judge, total_accumulate_judge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["min_judge_loss = float('inf')\n","\n","\n","for epoch, (train_idx, judge_idx) in enumerate(kfold.split(dataset_label)):\n","    print(f\"Starting epoch {epoch} with train size of {train_idx.shape} and judge size of {judge_idx.shape}\")\n","    total_loss_train, total_accumulate_train = train(train_idx)\n","    total_loss_judge, total_accumulate_judge = judge(judge_idx)\n","\n","    print(\n","        f'''Epochs: {epoch+1} \n","        | Train Loss: {total_loss_train / len(train_idx):.3f}\n","        | Train Accuracy: {total_accumulate_train/len(train_idx):.3f}\n","        | Val Loss: {total_loss_judge/len(judge_idx):.3f}\n","        | Val Accuracy: {total_accumulate_judge/len(judge_idx):.3f}'''\n","        \n","    )\n","    if min_judge_loss > total_loss_judge/len(judge_idx):\n","        min_judge_loss = total_loss_judge/len(judge_idx)\n","        torch.save(model.state_dict(), config.model_path)\n","        print(f\"Save model because val loss improve loss {min_judge_loss:.3f}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 ('nhatenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"867ad48846dc57cdb6a7846b46ef358b3169581697c80695711b9dc516eb64da"}}},"nbformat":4,"nbformat_minor":2}
